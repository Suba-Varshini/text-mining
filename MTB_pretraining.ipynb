{"cells":[{"cell_type":"markdown","metadata":{"id":"jSZcpHBesDNw"},"source":["# **BERT PAIR Relation Extraction Notebook**\n"]},{"cell_type":"markdown","metadata":{"id":"ndloW5ceTTDV"},"source":["## Imports and environment configuration"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"vWyMo4eeshmP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers==3.0.0"]},{"name":"stderr","output_type":"stream","text":["  error: subprocess-exited-with-error\n","  \n","  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n","  │ exit code: 1\n","  ╰─> [48 lines of output]\n","      C:\\Users\\elsab\\AppData\\Local\\Temp\\pip-build-env-l96wj4x7\\overlay\\Lib\\site-packages\\setuptools\\dist.py:314: InformationOnly: Normalizing '0.8.0.rc4' to '0.8.0rc4'\n","        self.metadata.version = self._normalize_version(self.metadata.version)\n","      running bdist_wheel\n","      running build\n","      running build_py\n","      creating build\n","      creating build\\lib.win-amd64-cpython-311\n","      creating build\\lib.win-amd64-cpython-311\\tokenizers\n","      copying tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\n","      creating build\\lib.win-amd64-cpython-311\\tokenizers\\models\n","      copying tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n","      creating build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n","      copying tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n","      creating build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n","      copying tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n","      creating build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n","      copying tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n","      creating build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n","      copying tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n","      creating build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n","      copying tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n","      creating build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n","      copying tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\n","      copying tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n","      copying tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n","      copying tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n","      copying tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n","      copying tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n","      copying tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n","      running build_ext\n","      running build_rust\n","      error: can't find Rust compiler\n","      \n","      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n","      \n","      To update pip, run:\n","      \n","          pip install --upgrade pip\n","      \n","      and then retry package installation.\n","      \n","      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n","      [end of output]\n","  \n","  note: This error originates from a subprocess, and is likely not a problem with pip.\n","  ERROR: Failed building wheel for tokenizers\n","ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"]},{"name":"stdout","output_type":"stream","text":["\n","  Using cached transformers-3.0.0-py3-none-any.whl.metadata (44 kB)\n","Requirement already satisfied: numpy in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from transformers==3.0.0) (1.26.0)\n","Collecting tokenizers==0.8.0-rc4 (from transformers==3.0.0)\n","  Using cached tokenizers-0.8.0rc4.tar.gz (96 kB)\n","  Installing build dependencies: started\n","  Installing build dependencies: finished with status 'done'\n","  Getting requirements to build wheel: started\n","  Getting requirements to build wheel: finished with status 'done'\n","  Preparing metadata (pyproject.toml): started\n","  Preparing metadata (pyproject.toml): finished with status 'done'\n","Requirement already satisfied: packaging in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from transformers==3.0.0) (23.1)\n","Requirement already satisfied: filelock in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from transformers==3.0.0) (3.13.1)\n","Requirement already satisfied: requests in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from transformers==3.0.0) (2.31.0)\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from transformers==3.0.0) (4.65.0)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from transformers==3.0.0) (2023.10.3)\n","Collecting sentencepiece (from transformers==3.0.0)\n","  Using cached sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n","Collecting sacremoses (from transformers==3.0.0)\n","  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: colorama in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from tqdm>=4.27->transformers==3.0.0) (0.4.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from requests->transformers==3.0.0) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from requests->transformers==3.0.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from requests->transformers==3.0.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from requests->transformers==3.0.0) (2024.2.2)\n","Collecting click (from sacremoses->transformers==3.0.0)\n","  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: joblib in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from sacremoses->transformers==3.0.0) (1.2.0)\n","Using cached transformers-3.0.0-py3-none-any.whl (754 kB)\n","Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","Using cached sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n","Using cached click-8.1.7-py3-none-any.whl (97 kB)\n","Building wheels for collected packages: tokenizers\n","  Building wheel for tokenizers (pyproject.toml): started\n","  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n","Failed to build tokenizers\n","Requirement already satisfied: ipython-autotime in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (0.3.2)\n","Requirement already satisfied: ipython in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from ipython-autotime) (8.20.0)\n","Requirement already satisfied: decorator in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from ipython->ipython-autotime) (5.1.1)\n","Requirement already satisfied: jedi>=0.16 in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from ipython->ipython-autotime) (0.18.1)\n","Requirement already satisfied: matplotlib-inline in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from ipython->ipython-autotime) (0.1.6)\n","Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from ipython->ipython-autotime) (3.0.43)\n","Requirement already satisfied: pygments>=2.4.0 in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from ipython->ipython-autotime) (2.15.1)\n","Requirement already satisfied: stack-data in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from ipython->ipython-autotime) (0.2.0)\n","Requirement already satisfied: traitlets>=5 in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from ipython->ipython-autotime) (5.7.1)\n","Requirement already satisfied: colorama in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from ipython->ipython-autotime) (0.4.6)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n","Requirement already satisfied: wcwidth in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->ipython-autotime) (0.2.5)\n","Requirement already satisfied: executing in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (0.8.3)\n","Requirement already satisfied: asttokens in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (2.0.5)\n","Requirement already satisfied: pure-eval in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (0.2.2)\n","Requirement already satisfied: six in c:\\users\\elsab\\miniconda3\\envs\\datacore\\lib\\site-packages (from asttokens->stack-data->ipython->ipython-autotime) (1.16.0)\n","The autotime extension is already loaded. To reload it, use:\n","  %reload_ext autotime\n","time: 8.86 s (started: 2024-03-06 16:45:00 +00:00)\n"]}],"source":["!pip install transformers==3.0.0\n","!pip install ipython-autotime\n","\n","%load_ext autotime"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"1nDTaiQhzRbO"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 16 ms (started: 2024-03-06 16:45:09 +00:00)\n"]}],"source":["import os\n","import sys\n","from pathlib import Path\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","basepath = Path(os.getcwd())\n","sys.path.append(os.path.join(basepath, 'models'))\n","sys.path.append(os.path.join(basepath, 'models', \"imported_configs\"))\n","\n","from model_files.modeling_bert import BertModel as Model\n","from tokens_files.tokenization_bert import BertTokenizer as Tokenizer"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"xUunXUqqSTNE"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 0 ns (started: 2024-03-06 16:45:09 +00:00)\n"]}],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["## Matching the Blanks Pre-Training"]},{"cell_type":"markdown","metadata":{},"source":["The pre-training process of Matching the Blanks can run for multiple days, even with GPU support. Therefore a already pre-trained model is provided in the GitLab repository. For additional information see README."]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.3.1/en_core_web_lg-2.3.1.tar.gz\n","\n","import en_core_web_lg"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 16 ms (started: 2024-03-06 16:12:07 +00:00)\n"]}],"source":["import os\n","import math\n","import time"]},{"cell_type":"markdown","metadata":{},"source":["### Pre-Training Helper functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 0 ns (started: 2024-03-06 16:12:07 +00:00)\n"]}],"source":["from imported_configs.helper_functions.pretrain_helper_functions import Two_Headed_Loss, pretrain_dataset, load_state, create_pretraining_corpus, process_textlines, mtb_evaluate_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 0 ns (started: 2024-03-06 16:12:07 +00:00)\n"]}],"source":["def mtb_load_dataloaders(pretrain_data, batch_size, max_length=50000):\n","    print(\"Loading pre-training data...\")\n","    with open(pretrain_data, \"r\", encoding=\"utf8\") as f:\n","        text = f.readlines()\n","    \n","    text = process_textlines(text)\n","    \n","    print(\"Length of text (characters): %d\" % len(text))\n","    num_chunks = math.ceil(len(text)/max_length)\n","    print(\"Splitting into %d max length chunks of size %d\" % (num_chunks, max_length))\n","    text_chunks = (text[i*max_length:(i*max_length + max_length)] for i in range(num_chunks))\n","    \n","    D = []\n","    print(\"Loading Spacy NLP...\")\n","    nlp = en_core_web_lg.load()\n","    \n","    for text_chunk in text_chunks:\n","        D.extend(create_pretraining_corpus(text_chunk, nlp, window_size=40))\n","        \n","    print(\"Total number of relation statements in pre-training corpus: %d\" % len(D))\n","\n","    train_set = pretrain_dataset(D, tokenizer, batch_size=batch_size)\n","    return train_set"]},{"cell_type":"markdown","metadata":{},"source":["### Pre-Training with Matching the Blanks"]},{"cell_type":"markdown","metadata":{},"source":["Definition of parameters for pre-training with Matching the Blanks"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 0 ns (started: 2024-03-06 16:12:07 +00:00)\n"]}],"source":["num_epochs=2\n","freeze=0\n","lr=0.0001\n","max_norm=1.0\n","gradient_acc_steps=2\n","batch_size=4\n","pretrain_data=os.path.join(basepath, \"fewrel-training-data\",\"cnn.txt\")\n","checkpoint_path = os.path.join(basepath, \"checkpoint_files\",\"pretrain_checkpoint_BERT_1.pth.tar\")"]},{"cell_type":"markdown","metadata":{},"source":["Loading model and tokenizer and initialize optimizer and scheduler for training routine."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 3.38 s (started: 2024-03-06 16:12:07 +00:00)\n"]}],"source":["model_name = 'bert-base-uncased'\n","lower_case=True\n","\n","tokenizer = Tokenizer.from_pretrained(model_name, do_lower_case=lower_case)\n","tokenizer.add_tokens(['[E1]', '[/E1]', '[E2]', '[/E2]', '[BLANK]'])\n","\n","mtb_model = Model.from_pretrained(model_name, force_download=False)\n","mtb_model.resize_token_embeddings(len(tokenizer)) \n","cuda = torch.cuda.is_available()\n","\n","if cuda:\n","    print(\"Cuda is on\")\n","    mtb_model.cuda()\n","\n","if freeze == 1:\n","    print(\"FREEZING MOST HIDDEN LAYERS...\")\n","    unfrozen_layers = [\"classifier\", \"pooler\", \"encoder.layer.11\", \"encoder.layer.10\",\\\n","                        \"encoder.layer.9\", \"blanks_linear\", \"lm_linear\", \"cls\"]\n","        \n","    for name, param in mtb_model.named_parameters():\n","        if not any([layer in name for layer in unfrozen_layers]):\n","            print(\"[FROZE]: %s\" % name)\n","            param.requires_grad = False\n","        else:\n","            print(\"[FREE]: %s\" % name)\n","            param.requires_grad = True\n","    \n","criterion = Two_Headed_Loss(lm_ignore_idx=tokenizer.pad_token_id, use_logits=True, normalize=False)\n","optimizer = optim.Adam([{\"params\":mtb_model.parameters(), \"lr\": lr}])\n","\n","scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2,4,6,8,12,15,18,20,22,24,26,30], gamma=0.8)"]},{"cell_type":"markdown","metadata":{},"source":["Loading pre-training data from inputfile."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading pre-training data...\n","Length of text (characters): 1041308\n","Splitting into 21 max length chunks of size 50000\n","Loading Spacy NLP...\n","Total number of relation statements in pre-training corpus: 14646\n","time: 36.6 s (started: 2024-03-06 16:12:11 +00:00)\n"]}],"source":["train_loader = mtb_load_dataloaders(pretrain_data, batch_size)\n","train_len = len(train_loader)"]},{"cell_type":"markdown","metadata":{},"source":["Load checkpoint if available to continue training from this point."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded checkpoint model.\n","Loaded model and optimizer.\n","time: 1.38 s (started: 2024-03-06 16:12:47 +00:00)\n"]}],"source":["start_epoch, best_pred = load_state(mtb_model, optimizer, scheduler, checkpoint_path)"]},{"cell_type":"markdown","metadata":{},"source":["Pre-Training process with Matching the Blank method. Caution: This kind of training runs at least 10 to 12 hours on limited hardware."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting training process...\n","Finished Training!\n","time: 0 ns (started: 2024-03-06 16:12:49 +00:00)\n"]}],"source":["losses_per_epoch=[]\n","accuracy_per_epoch=[]\n","\n","print(\"Starting training process...\")\n","pad_id = tokenizer.pad_token_id\n","mask_id = tokenizer.mask_token_id\n","update_size = len(train_loader)//10\n","\n","for epoch in range(start_epoch, num_epochs):\n","    start_time = time.time()\n","    mtb_model.train(); total_loss = 0.0; losses_per_batch = []; total_acc = 0.0; lm_accuracy_per_batch = []\n","    for i, data in enumerate(train_loader, 0):\n","        x, masked_for_pred, e1_e2_start, _, blank_labels, _,_,_,_,_ = data\n","        masked_for_pred1 =  masked_for_pred\n","        masked_for_pred = masked_for_pred[(masked_for_pred != pad_id)]\n","        if masked_for_pred.shape[0] == 0:\n","            print('Empty dataset, skipping...')\n","            continue\n","        attention_mask = (x != pad_id).float()\n","        token_type_ids = torch.zeros((x.shape[0], x.shape[1])).long()\n","\n","        if cuda:\n","            x = x.cuda(); masked_for_pred = masked_for_pred.cuda()\n","            attention_mask = attention_mask.cuda()\n","            token_type_ids = token_type_ids.cuda()\n","        \n","        blanks_logits, lm_logits, _ = mtb_model(x, token_type_ids=token_type_ids, attention_mask=attention_mask, Q=None,\\\n","                      e1_e2_start=e1_e2_start)\n","        lm_logits = lm_logits[(x == mask_id)]\n","        \n","        if (i % update_size) == (update_size - 1):\n","            verbose = True\n","        else:\n","            verbose = False\n","            \n","        loss = criterion(lm_logits, blanks_logits, masked_for_pred, blank_labels, verbose=verbose)\n","        loss = loss/gradient_acc_steps\n","\n","        loss.backward()\n","\n","        grad_norm = nn.utils.clip_grad_norm_(mtb_model.parameters(), max_norm)\n","        \n","        if (i % gradient_acc_steps) == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","        \n","        total_loss += loss.item()\n","        total_acc += mtb_evaluate_(lm_logits, blanks_logits, masked_for_pred, blank_labels, \\\n","                                tokenizer, print_=False)[0]\n","        \n","        if (i % update_size) == (update_size - 1):\n","            losses_per_batch.append(gradient_acc_steps*total_loss/update_size)\n","            lm_accuracy_per_batch.append(total_acc/update_size)\n","            print('[Epoch: %d, %5d/ %d points] total loss, lm accuracy per batch: %.3f, %.3f' %\n","                  (epoch + 1, (i + 1), train_len, losses_per_batch[-1], lm_accuracy_per_batch[-1]))\n","            total_loss = 0.0; total_acc = 0.0\n","            print(\"Last batch samples (pos, neg): %d, %d\" % ((blank_labels.squeeze() == 1).sum().item(),\\\n","                                                                (blank_labels.squeeze() == 0).sum().item()))\n","    \n","    scheduler.step()\n","    losses_per_epoch.append(sum(losses_per_batch)/len(losses_per_batch))\n","    accuracy_per_epoch.append(sum(lm_accuracy_per_batch)/len(lm_accuracy_per_batch))\n","    print(\"Losses at Epoch %d: %.7f\" % (epoch + 1, losses_per_epoch[-1]))\n","    print(\"Accuracy at Epoch %d: %.7f\" % (epoch + 1, accuracy_per_epoch[-1]))\n","    \n","    torch.save({\n","            'epoch': epoch + 1,\\\n","            'state_dict': mtb_model.state_dict(),\\\n","            'best_acc': accuracy_per_epoch[-1],\\\n","            'optimizer' : optimizer.state_dict(),\\\n","            'scheduler' : scheduler.state_dict(),\\\n","            'amp': None\n","        }, os.path.join(basepath, \"checkpoint_files\",\"pretrain\",\"pretrain_checkpoint_BERT_1.pth.tar\"))\n","\n","print(\"Finished Training!\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMo8XTsfKLEzXTJjW54y5wo","collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1H0MsHAw6tTnoH6SUzELHAdoXHxVD0vkV","name":"BERT PAIR Relation Extraction.ipynb","provenance":[{"file_id":"1S-npM9Umn_s4D3FS6Fyl_pEo5fopcuob","timestamp":1591960614367},{"file_id":"1_i6d5uaAiqpuUwvf5NiHxcXHf58kAqt1","timestamp":1591254255357}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
