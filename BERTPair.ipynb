{"cells":[{"cell_type":"markdown","metadata":{"id":"jSZcpHBesDNw"},"source":["# **BERT PAIR Relation Extraction Notebook**\n"]},{"cell_type":"markdown","metadata":{},"source":["This notebook contains the Bert-Pair approach to relation extraction. It should only be run after pretraining the model using the MTB_pretraining.ipynb file as we are trying to produce a hybrid model that is pretrained on MTB and then trained and evaluated on BERT-Pair."]},{"cell_type":"markdown","metadata":{},"source":["## Importing files and libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"vWyMo4eeshmP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers==3.0.0\n","  Using cached transformers-3.0.0-py3-none-any.whl.metadata (44 kB)\n","Requirement already satisfied: numpy in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from transformers==3.0.0) (1.26.0)\n","Collecting tokenizers==0.8.0-rc4 (from transformers==3.0.0)\n","  Using cached tokenizers-0.8.0rc4.tar.gz (96 kB)\n","  Installing build dependencies: started\n","  Installing build dependencies: finished with status 'done'\n","  Getting requirements to build wheel: started\n","  Getting requirements to build wheel: finished with status 'done'\n","  Preparing metadata (pyproject.toml): started\n","  Preparing metadata (pyproject.toml): finished with status 'done'\n","Requirement already satisfied: packaging in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from transformers==3.0.0) (23.1)\n","Requirement already satisfied: filelock in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from transformers==3.0.0) (3.13.1)\n","Requirement already satisfied: requests in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from transformers==3.0.0) (2.28.1)\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from transformers==3.0.0) (4.65.0)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from transformers==3.0.0) (2023.10.3)\n","Collecting sentencepiece (from transformers==3.0.0)\n","  Using cached sentencepiece-0.2.0-cp39-cp39-win_amd64.whl.metadata (8.3 kB)\n","Collecting sacremoses (from transformers==3.0.0)\n","  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: colorama in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from tqdm>=4.27->transformers==3.0.0) (0.4.6)\n","Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from requests->transformers==3.0.0) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from requests->transformers==3.0.0) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from requests->transformers==3.0.0) (1.26.13)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from requests->transformers==3.0.0) (2024.2.2)\n","Collecting click (from sacremoses->transformers==3.0.0)\n","  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: joblib in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from sacremoses->transformers==3.0.0) (1.2.0)\n","Using cached transformers-3.0.0-py3-none-any.whl (754 kB)\n","Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","Using cached sentencepiece-0.2.0-cp39-cp39-win_amd64.whl (991 kB)\n","Using cached click-8.1.7-py3-none-any.whl (97 kB)\n","Building wheels for collected packages: tokenizers\n","  Building wheel for tokenizers (pyproject.toml): started\n","  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n","Failed to build tokenizers\n"]},{"name":"stderr","output_type":"stream","text":["  error: subprocess-exited-with-error\n","  \n","  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n","  │ exit code: 1\n","  ╰─> [48 lines of output]\n","      C:\\Users\\elsab\\AppData\\Local\\Temp\\pip-build-env-ikq2vhsv\\overlay\\Lib\\site-packages\\setuptools\\dist.py:314: InformationOnly: Normalizing '0.8.0.rc4' to '0.8.0rc4'\n","        self.metadata.version = self._normalize_version(self.metadata.version)\n","      running bdist_wheel\n","      running build\n","      running build_py\n","      creating build\n","      creating build\\lib.win-amd64-cpython-39\n","      creating build\\lib.win-amd64-cpython-39\\tokenizers\n","      copying tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\n","      creating build\\lib.win-amd64-cpython-39\\tokenizers\\models\n","      copying tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n","      creating build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n","      copying tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n","      creating build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n","      copying tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n","      creating build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n","      copying tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n","      creating build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n","      copying tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n","      creating build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n","      copying tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n","      creating build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n","      copying tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\n","      copying tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n","      copying tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n","      copying tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n","      copying tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n","      copying tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n","      copying tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n","      running build_ext\n","      running build_rust\n","      error: can't find Rust compiler\n","      \n","      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n","      \n","      To update pip, run:\n","      \n","          pip install --upgrade pip\n","      \n","      and then retry package installation.\n","      \n","      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n","      [end of output]\n","  \n","  note: This error originates from a subprocess, and is likely not a problem with pip.\n","  ERROR: Failed building wheel for tokenizers\n","ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: ipython-autotime in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (0.3.2)\n","Requirement already satisfied: ipython in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython-autotime) (8.15.0)\n","Requirement already satisfied: backcall in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (0.2.0)\n","Requirement already satisfied: decorator in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (5.1.1)\n","Requirement already satisfied: jedi>=0.16 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (0.18.1)\n","Requirement already satisfied: matplotlib-inline in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (0.1.6)\n","Requirement already satisfied: pickleshare in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (3.0.36)\n","Requirement already satisfied: pygments>=2.4.0 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (2.15.1)\n","Requirement already satisfied: stack-data in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (0.2.0)\n","Requirement already satisfied: traitlets>=5 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (5.7.1)\n","Requirement already satisfied: typing-extensions in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (4.7.1)\n","Requirement already satisfied: exceptiongroup in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (1.0.4)\n","Requirement already satisfied: colorama in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (0.4.6)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n","Requirement already satisfied: wcwidth in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->ipython-autotime) (0.2.5)\n","Requirement already satisfied: executing in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (0.8.3)\n","Requirement already satisfied: asttokens in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (2.0.5)\n","Requirement already satisfied: pure-eval in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (0.2.2)\n","Requirement already satisfied: six in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from asttokens->stack-data->ipython->ipython-autotime) (1.16.0)\n","time: 0 ns (started: 2024-03-08 02:39:18 +00:00)\n"]}],"source":["!pip install transformers==3.0.0\n","!pip install ipython-autotime\n","\n","%load_ext autotime"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"1nDTaiQhzRbO"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["time: 1.19 s (started: 2024-03-08 02:39:18 +00:00)\n"]}],"source":["import os\n","import sys\n","import random\n","import json\n","from pathlib import Path\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from transformers import BertTokenizer, BertForSequenceClassification\n","\n","\n","basepath = Path(os.getcwd())\n","sys.path.append(os.path.join(basepath, 'models'))\n","from framework import FewShotREFramework, FewShotREModel\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 0 ns (started: 2024-03-08 02:39:20 +00:00)\n"]}],"source":["# defining paths for datasets and pretrained checkpoint file\n","checkpoint_path = os.path.join(basepath, \"checkpoint_files\",\"pretrain_checkpoint_BERT_1.pth.tar\")\n","\n","data_dir = os.path.join( os.getcwd(),'fewrel-training-data/')\n","train_file = os.path.join(data_dir, 'train_wiki')\n","val_file = os.path.join(data_dir, 'val_wiki')\n","test_file = os.path.join(data_dir, 'val_pubmed')\n"]},{"cell_type":"markdown","metadata":{},"source":["# BERT PAIR"]},{"cell_type":"markdown","metadata":{"id":"EQWk8iPigdfv"},"source":["This portion of the code is adapted from the Few_rel official repository, this contains the encoder, the model, and the data loader of the datasets. "]},{"cell_type":"code","execution_count":4,"metadata":{"id":"0U4gpuIFRpjP"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 0 ns (started: 2024-03-08 02:39:20 +00:00)\n"]}],"source":["from torch import optim, nn\n","from torch.nn import functional as F\n","\n","class BERTPAIRSentenceEncoder(nn.Module):\n","    def __init__(self, pretrain_path, max_length): \n","        nn.Module.__init__(self)\n","        # we initialize the Bert model from huggingface with the pretrained bert-base-uncased \n","        # The encoder is our main model\n","        self.bert = BertForSequenceClassification.from_pretrained(\n","                pretrain_path)\n","        self.max_length = max_length\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    def forward(self, inputs):\n","        # Encoder's main function, it passes the tokenified data to the bert model\n","        x = self.bert(inputs['word'], token_type_ids=inputs['seg'], attention_mask=inputs['mask'])[0]\n","        return x\n","    \n","    def tokenize(self, raw_tokens, pos_head, pos_tail):\n","        tokens = []\n","        cur_pos = 0\n","        pos1_in_index = 0\n","        pos2_in_index = 0\n","        for token in raw_tokens:\n","            token = token.lower()\n","            if cur_pos == pos_head[0]:\n","                tokens.append('[unused0]')\n","                pos1_in_index = len(tokens)\n","            if cur_pos == pos_tail[0]:\n","                tokens.append('[unused1]')\n","                pos2_in_index = len(tokens)\n","            tokens += self.tokenizer.tokenize(token)\n","            if cur_pos == pos_head[-1]:\n","                tokens.append('[unused2]')\n","            if cur_pos == pos_tail[-1]:\n","                tokens.append('[unused3]')\n","            cur_pos += 1\n","        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokens)\n","        \n","        return indexed_tokens\n","\n","\n","class Pair(FewShotREModel):\n","    \n","    def __init__(self, sentence_encoder, hidden_size=8):\n","        FewShotREModel.__init__(self, sentence_encoder)\n","        self.hidden_size = hidden_size\n","        self.drop = nn.Dropout()\n","\n","    def forward(self, batch, N, K, total_Q):\n","        '''\n","        support: Inputs of the support set.\n","        query: Inputs of the query set.\n","        N: Num of classes\n","        K: Num of instances for each class in the support set\n","        Q: Num of instances in the query set\n","        '''\n","    \n","        logits = self.sentence_encoder(batch)\n","        logits = logits.view(-1, total_Q, N, K, 2)\n","        logits = logits.mean(3) # (-1, total_Q, N, 2)\n","\n","        logits_na, _ = logits[:, :, :, 0].min(2, keepdim=True) # (-1, totalQ, 1)\n","        logits = logits[:, : , :, 1] # (-1, total_Q, N)\n","        logits = torch.cat([logits, logits_na], 2) # (B, total_Q, N + 1)\n","        _, pred = torch.max(logits.view(-1, N+1), 1)\n","        return logits, pred\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 0 ns (started: 2024-03-08 02:39:20 +00:00)\n"]}],"source":["import torch.utils.data as data\n","# Official Data loader of the fewrel dataset, the datasets can be found in the folder fewrel-training-data\n","# under the form of json files.\n","\n","class FewRelDatasetPair(data.Dataset):\n","    \"\"\"\n","    FewRel Pair Dataset\n","    \"\"\"\n","    def __init__(self, name, encoder, N, K, Q, na_rate, root, encoder_name):\n","\n","        self.root = root\n","        path = os.path.join(root, name + \".json\")\n","        if not os.path.exists(path):\n","            print(\"[ERROR] Data file does not exist!\")\n","            assert(0)\n","\n","        self.json_data = json.load(open(path))\n","\n","        self.classes = list(self.json_data.keys())\n","        self.N = N\n","        self.K = K\n","        self.Q = Q\n","        self.na_rate = na_rate\n","        self.encoder = encoder\n","        self.encoder_name = encoder_name\n","        self.max_length = encoder.max_length\n","\n","    def __getraw__(self, item):\n","        word = self.encoder.tokenize(item['tokens'],\n","            item['h'][2][0],\n","            item['t'][2][0])\n","        return word \n","\n","    def __additem__(self, d, word, pos1, pos2, mask):\n","        d['word'].append(word)\n","        d['pos1'].append(pos1)\n","        d['pos2'].append(pos2)\n","        d['mask'].append(mask)\n","\n","    def __getitem__(self, index):\n","        target_classes = random.sample(self.classes, self.N)\n","        support = []\n","        query = []\n","        fusion_set = {'word': [], 'mask': [], 'seg': []}\n","        query_label = []\n","        Q_na = int(self.na_rate * self.Q)\n","        na_classes = list(filter(lambda x: x not in target_classes,  \n","            self.classes))\n","\n","        for i, class_name in enumerate(target_classes):\n","            indices = np.random.choice(\n","                    list(range(len(self.json_data[class_name]))), \n","                    self.K + self.Q, False)\n","            count = 0\n","            for j in indices:\n","                word  = self.__getraw__(\n","                        self.json_data[class_name][j])\n","                if count < self.K:\n","                    support.append(word)\n","                else:\n","                    query.append(word)\n","                count += 1\n","\n","            query_label += [i] * self.Q\n","\n","        # NA\n","        for j in range(Q_na):\n","            cur_class = np.random.choice(na_classes, 1, False)[0]\n","            index = np.random.choice(\n","                    list(range(len(self.json_data[cur_class]))),\n","                    1, False)[0]\n","            word = self.__getraw__(\n","                    self.json_data[cur_class][index])\n","            query.append(word)\n","        query_label += [self.N] * Q_na\n","\n","        for word_query in query:\n","            for word_support in support:\n","                if self.encoder_name == 'bert':\n","                    SEP = self.encoder.tokenizer.convert_tokens_to_ids(['[SEP]'])\n","                    CLS = self.encoder.tokenizer.convert_tokens_to_ids(['[CLS]'])\n","                    word_tensor = torch.zeros((self.max_length)).long()\n","                else:\n","                    SEP = self.encoder.tokenizer.convert_tokens_to_ids(['</s>'])     \n","                    CLS = self.encoder.tokenizer.convert_tokens_to_ids(['<s>'])\n","                    word_tensor = torch.ones((self.max_length)).long()\n","                new_word = CLS + word_support + SEP + word_query + SEP\n","                for i in range(min(self.max_length, len(new_word))):\n","                    word_tensor[i] = new_word[i]\n","                mask_tensor = torch.zeros((self.max_length)).long()\n","                mask_tensor[:min(self.max_length, len(new_word))] = 1\n","                seg_tensor = torch.ones((self.max_length)).long()\n","                seg_tensor[:min(self.max_length, len(word_support) + 1)] = 0\n","                fusion_set['word'].append(word_tensor)\n","                fusion_set['mask'].append(mask_tensor)\n","                fusion_set['seg'].append(seg_tensor)\n","\n","        return fusion_set, query_label\n","    \n","    def __len__(self):\n","        return 10000000\n","        \n","def collate_fn_pair(data):\n","    batch_set = {'word': [], 'seg': [], 'mask': []}\n","    batch_label = []\n","    fusion_sets, query_labels = zip(*data)\n","    for i in range(len(fusion_sets)):\n","        for k in fusion_sets[i]:\n","            batch_set[k] += fusion_sets[i][k]\n","        batch_label += query_labels[i]\n","    for k in batch_set:\n","        batch_set[k] = torch.stack(batch_set[k], 0)\n","    batch_label = torch.tensor(batch_label)\n","    return batch_set, batch_label\n","\n","def get_loader_pair(name, encoder, N, K, Q, batch_size, \n","        num_workers=0, collate_fn=collate_fn_pair, na_rate=0, root='./data', encoder_name='bert'):\n","    dataset = FewRelDatasetPair(name, encoder, N, K, Q, na_rate, root, encoder_name)\n","    data_loader = torch.utils.data.DataLoader(dataset=dataset,\n","            batch_size=batch_size,\n","            shuffle=False,\n","            pin_memory=True,\n","            num_workers=num_workers,\n","            collate_fn=collate_fn)\n","    return iter(data_loader)"]},{"cell_type":"markdown","metadata":{},"source":["## Training section"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Nh7ojhQcQ49C"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 0 ns (started: 2024-03-08 02:39:20 +00:00)\n"]}],"source":["## Fine-Tuning and paths Parameters for the fewshot framework\n","trainN = 5\n","N = 5\n","K = 1\n","Q = 1\n","batch_size = 4\n","max_length = 3\n","hidden_size = 3\n","na_rate = 5\n","\n","val_step = 200\n","train_iter = 500\n","val_iter = 100\n","test_iter = 1000\n","\n","# Checkpoint files for the training phase\n","ckpt = os.path.join(basepath, 'checkpoint_files','bert-pair-fewrel.pth.tar')\n","prefix = 'bert-pair-fewrel.pth.tar'"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"jRN6DIdIRWrk"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["time: 1.31 s (started: 2024-03-08 02:39:20 +00:00)\n"]}],"source":["# We initialize our encoder and model\n","sentence_encoder = BERTPAIRSentenceEncoder('bert-base-uncased', max_length)\n","model = Pair(sentence_encoder, hidden_size=hidden_size)"]},{"cell_type":"markdown","metadata":{"id":"seYo0E1Hf8vo"},"source":["Loading train-, validation- and test-data and initializing the FewShotREFramework with the different data loaders"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"aG4Ht8KJYsvb"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 781 ms (started: 2024-03-08 02:39:21 +00:00)\n"]}],"source":["# Loading our three datasets, one for training, one for validation and one for testing \n","train_data_loader = get_loader_pair(train_file, sentence_encoder, N=trainN, K=K, Q=Q, na_rate=na_rate, batch_size=batch_size, encoder_name='bert')\n","val_data_loader = get_loader_pair(val_file, sentence_encoder, N=N, K=K, Q=Q, na_rate=na_rate, batch_size=batch_size, encoder_name='bert')\n","test_data_loader = get_loader_pair(test_file, sentence_encoder, N=N, K=K, Q=Q, na_rate=na_rate, batch_size=batch_size, encoder_name='bert')\n","\n","# Initializing the few shot framework\n","framework = FewShotREFramework(train_data_loader, val_data_loader, test_data_loader)"]},{"cell_type":"markdown","metadata":{"id":"XJhP6ew3fe1-"},"source":["Training the model using the provided FewShotREFramework from the authors of the FewRel dataset"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"foOVYGzHUuf9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start training...\n","Use bert optim!\n","Successfully loaded checkpoint 'c:\\Users\\elsab\\Documents\\git\\text-mining\\checkpoint_files\\pretrain_checkpoint_BERT_1.pth.tar'\n","-------------------------------\n","Evaluation with evaluation data\n","[EVAL] step:  100 | accuracy: 12.90%\n","   Evaluation accuracy: 0.129000\n","Best checkpoint -> Saving checkpoint with accuracy of 0.129000\n","step:  100 | loss: 1.553072, accuracy: 46.67%\n","step:  200 | loss: 1.529634, accuracy: 48.33%\n","-------------------------------\n","Evaluation with evaluation data\n","[EVAL] step:  100 | accuracy: 50.00%\n","   Evaluation accuracy: 0.500000\n","Best checkpoint -> Saving checkpoint with accuracy of 0.500000\n","step:  300 | loss: 1.507452, accuracy: 50.00%\n","step:  400 | loss: 1.505783, accuracy: 50.00%\n","-------------------------------\n","Evaluation with evaluation data\n","[EVAL] step:  100 | accuracy: 50.00%\n","   Evaluation accuracy: 0.500000\n","step:  500 | loss: 1.500024, accuracy: 50.00%\n","\n","####################\n","\n","Finish training bert-pair-fewrel.pth.tar\n","Evaluation with Test data\n","Successfully loaded checkpoint 'c:\\Users\\elsab\\Documents\\git\\text-mining\\checkpoint_files\\bert-pair-fewrel.pth.tar'\n","[TEST] step:  100 | accuracy: 50.00%\n","[TEST] step:  200 | accuracy: 50.00%\n","[TEST] step:  300 | accuracy: 50.00%\n","[TEST] step:  400 | accuracy: 50.00%\n","[TEST] step:  500 | accuracy: 50.00%\n","[TEST] step:  600 | accuracy: 50.00%\n","[TEST] step:  700 | accuracy: 50.00%\n","[TEST] step:  800 | accuracy: 50.00%\n","[TEST] step:  900 | accuracy: 50.00%\n","[TEST] step: 1000 | accuracy: 50.00%\n","[TEST] step: 1100 | accuracy: 50.00%\n","[TEST] step: 1200 | accuracy: 50.00%\n","[TEST] step: 1300 | accuracy: 50.00%\n","[TEST] step: 1400 | accuracy: 50.00%\n","[TEST] step: 1500 | accuracy: 50.00%\n","[TEST] step: 1600 | accuracy: 50.00%\n","[TEST] step: 1700 | accuracy: 50.00%\n","[TEST] step: 1800 | accuracy: 50.00%\n","[TEST] step: 1900 | accuracy: 50.00%\n","[TEST] step: 2000 | accuracy: 50.00%\n","[TEST] step: 2100 | accuracy: 50.00%\n","[TEST] step: 2200 | accuracy: 50.00%\n","[TEST] step: 2300 | accuracy: 50.00%\n","[TEST] step: 2400 | accuracy: 50.00%\n","[TEST] step: 2500 | accuracy: 50.00%\n","[TEST] step: 2600 | accuracy: 50.00%\n","[TEST] step: 2700 | accuracy: 50.00%\n","[TEST] step: 2800 | accuracy: 50.00%\n","[TEST] step: 2900 | accuracy: 50.00%\n","[TEST] step: 3000 | accuracy: 50.00%\n","Test accuracy: 0.500000\n","time: 19min 40s (started: 2024-03-08 02:39:22 +00:00)\n"]}],"source":["# Training of the model using the few shot framework\n","# we load the mtb pretrained model on it first.\n","\n","framework.train(model, prefix, batch_size, trainN, N, K, Q,\n","        pytorch_optim=optim.SGD, na_rate=na_rate, val_step=val_step, pair=True, \n","        train_iter=train_iter, val_iter=val_iter, bert_optim=True,\n","        save_ckpt=ckpt, load_ckpt=checkpoint_path)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMo8XTsfKLEzXTJjW54y5wo","collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1H0MsHAw6tTnoH6SUzELHAdoXHxVD0vkV","name":"BERT PAIR Relation Extraction.ipynb","provenance":[{"file_id":"1S-npM9Umn_s4D3FS6Fyl_pEo5fopcuob","timestamp":1591960614367},{"file_id":"1_i6d5uaAiqpuUwvf5NiHxcXHf58kAqt1","timestamp":1591254255357}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
