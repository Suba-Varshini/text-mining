{"cells":[{"cell_type":"markdown","metadata":{"id":"jSZcpHBesDNw"},"source":["# **BERT PAIR Relation Extraction Notebook**\n"]},{"cell_type":"markdown","metadata":{"id":"ndloW5ceTTDV"},"source":["## Imports and environment configuration"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"vWyMo4eeshmP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers==3.0.0\n","  Using cached transformers-3.0.0-py3-none-any.whl.metadata (44 kB)\n","Requirement already satisfied: numpy in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from transformers==3.0.0) (1.26.0)\n","Collecting tokenizers==0.8.0-rc4 (from transformers==3.0.0)\n","  Using cached tokenizers-0.8.0rc4.tar.gz (96 kB)\n","  Installing build dependencies: started\n","  Installing build dependencies: finished with status 'done'\n","  Getting requirements to build wheel: started\n","  Getting requirements to build wheel: finished with status 'done'\n","  Preparing metadata (pyproject.toml): started\n","  Preparing metadata (pyproject.toml): finished with status 'done'\n","Requirement already satisfied: packaging in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from transformers==3.0.0) (23.1)\n","Requirement already satisfied: filelock in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from transformers==3.0.0) (3.13.1)\n","Requirement already satisfied: requests in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from transformers==3.0.0) (2.28.1)\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from transformers==3.0.0) (4.65.0)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from transformers==3.0.0) (2023.10.3)\n","Collecting sentencepiece (from transformers==3.0.0)\n","  Using cached sentencepiece-0.2.0-cp39-cp39-win_amd64.whl.metadata (8.3 kB)\n","Collecting sacremoses (from transformers==3.0.0)\n","  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: colorama in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from tqdm>=4.27->transformers==3.0.0) (0.4.6)\n","Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from requests->transformers==3.0.0) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from requests->transformers==3.0.0) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from requests->transformers==3.0.0) (1.26.13)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from requests->transformers==3.0.0) (2024.2.2)\n","Collecting click (from sacremoses->transformers==3.0.0)\n","  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: joblib in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from sacremoses->transformers==3.0.0) (1.2.0)\n","Using cached transformers-3.0.0-py3-none-any.whl (754 kB)\n","Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","Using cached sentencepiece-0.2.0-cp39-cp39-win_amd64.whl (991 kB)\n","Using cached click-8.1.7-py3-none-any.whl (97 kB)\n","Building wheels for collected packages: tokenizers\n","  Building wheel for tokenizers (pyproject.toml): started\n","  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n","Failed to build tokenizers\n"]},{"name":"stderr","output_type":"stream","text":["  error: subprocess-exited-with-error\n","  \n","  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n","  │ exit code: 1\n","  ╰─> [48 lines of output]\n","      C:\\Users\\elsab\\AppData\\Local\\Temp\\pip-build-env-u7530r7c\\overlay\\Lib\\site-packages\\setuptools\\dist.py:314: InformationOnly: Normalizing '0.8.0.rc4' to '0.8.0rc4'\n","        self.metadata.version = self._normalize_version(self.metadata.version)\n","      running bdist_wheel\n","      running build\n","      running build_py\n","      creating build\n","      creating build\\lib.win-amd64-cpython-39\n","      creating build\\lib.win-amd64-cpython-39\\tokenizers\n","      copying tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\n","      creating build\\lib.win-amd64-cpython-39\\tokenizers\\models\n","      copying tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n","      creating build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n","      copying tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n","      creating build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n","      copying tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n","      creating build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n","      copying tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n","      creating build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n","      copying tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n","      creating build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n","      copying tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n","      creating build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n","      copying tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n","      copying tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\n","      copying tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n","      copying tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n","      copying tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n","      copying tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n","      copying tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n","      copying tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n","      running build_ext\n","      running build_rust\n","      error: can't find Rust compiler\n","      \n","      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n","      \n","      To update pip, run:\n","      \n","          pip install --upgrade pip\n","      \n","      and then retry package installation.\n","      \n","      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n","      [end of output]\n","  \n","  note: This error originates from a subprocess, and is likely not a problem with pip.\n","  ERROR: Failed building wheel for tokenizers\n","ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: ipython-autotime in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (0.3.2)\n","Requirement already satisfied: ipython in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython-autotime) (8.15.0)\n","Requirement already satisfied: backcall in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (0.2.0)\n","Requirement already satisfied: decorator in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (5.1.1)\n","Requirement already satisfied: jedi>=0.16 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (0.18.1)\n","Requirement already satisfied: matplotlib-inline in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (0.1.6)\n","Requirement already satisfied: pickleshare in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (3.0.36)\n","Requirement already satisfied: pygments>=2.4.0 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (2.15.1)\n","Requirement already satisfied: stack-data in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (0.2.0)\n","Requirement already satisfied: traitlets>=5 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (5.7.1)\n","Requirement already satisfied: typing-extensions in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (4.7.1)\n","Requirement already satisfied: exceptiongroup in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (1.0.4)\n","Requirement already satisfied: colorama in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from ipython->ipython-autotime) (0.4.6)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n","Requirement already satisfied: wcwidth in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->ipython-autotime) (0.2.5)\n","Requirement already satisfied: executing in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (0.8.3)\n","Requirement already satisfied: asttokens in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (2.0.5)\n","Requirement already satisfied: pure-eval in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (0.2.2)\n","Requirement already satisfied: six in c:\\users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages (from asttokens->stack-data->ipython->ipython-autotime) (1.16.0)\n","time: 0 ns (started: 2024-03-06 22:00:04 +00:00)\n"]}],"source":["!pip install transformers==3.0.0\n","!pip install ipython-autotime\n","\n","%load_ext autotime"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"1nDTaiQhzRbO"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\elsab\\miniconda3\\envs\\data2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["Running locally\n","time: 10.2 s (started: 2024-03-06 22:00:04 +00:00)\n"]}],"source":["import os\n","import sys\n","import json\n","import random\n","from pathlib import Path\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from transformers import BertTokenizer, BertForSequenceClassification\n","\n","\n","\n","basepath = Path(os.getcwd())\n","\n","\n","print('Running locally')\n","root = Path(os.getcwd())\n","basepath = Path(os.getcwd())\n","sys.path.append(os.path.join(basepath, 'models', \"imported_configs\"))\n","\n","from model_files.modeling_bert import BertModel as Model\n","from tokens_files.tokenization_bert import BertTokenizer as Tokenizer\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"xUunXUqqSTNE"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 15 ms (started: 2024-03-06 22:00:14 +00:00)\n"]}],"source":["sys.path.append(os.path.join(basepath, 'models'))\n","\n","from pair import Pair\n","from framework import FewShotREFramework\n","import warnings\n","warnings.filterwarnings('ignore')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Matching the Blanks Pre-Training"]},{"cell_type":"markdown","metadata":{},"source":["The pre-training process of Matching the Blanks can run for multiple days, even with GPU support. Therefore a already pre-trained model is provided in the GitLab repository. For additional information see README."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 406 ms (started: 2024-03-06 22:00:14 +00:00)\n"]}],"source":["import en_core_web_lg"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 0 ns (started: 2024-03-06 22:00:15 +00:00)\n"]}],"source":["import os\n","import math\n","import time\n","import json\n","from pathlib import Path"]},{"cell_type":"markdown","metadata":{},"source":["Definition of parameters for pre-training with Matching the Blanks"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 0 ns (started: 2024-03-06 22:00:15 +00:00)\n"]}],"source":["num_epochs=1\n","freeze=0\n","lr=0.0001\n","max_norm=1.0\n","gradient_acc_steps=2\n","batch_size=4\n","checkpoint_path = os.path.join(basepath, \"checkpoint_files\",\"pretrain_checkpoint_BERT_1.pth.tar\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"CyGIiDcqB1iy"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 0 ns (started: 2024-03-06 22:00:15 +00:00)\n"]}],"source":["\n","data_dir = os.path.join( os.getcwd(),'fewrel-training-data/')\n","\n","train_file = os.path.join(data_dir, 'train_wiki')\n","val_file = os.path.join(data_dir, 'val_wiki')\n","test_file = os.path.join(data_dir, 'val_pubmed')\n"]},{"cell_type":"markdown","metadata":{},"source":["# BERT PAIR"]},{"cell_type":"markdown","metadata":{"id":"EQWk8iPigdfv"},"source":["Sentence encoder class for the BERT Pair approach which manages the model and the tokenizer"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"0U4gpuIFRpjP"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 0 ns (started: 2024-03-06 22:00:15 +00:00)\n"]}],"source":["class BERTPAIRSentenceEncoder(nn.Module):\n","    def __init__(self, pretrain_path, max_length): \n","        nn.Module.__init__(self)\n","        self.bert = BertForSequenceClassification.from_pretrained(\n","                pretrain_path)\n","        self.max_length = max_length\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    def forward(self, inputs):\n","        x = self.bert(inputs['word'], token_type_ids=inputs['seg'], attention_mask=inputs['mask'])[0]\n","        return x\n","    \n","    def tokenize(self, raw_tokens, pos_head, pos_tail):\n","        # token -> index\n","        # tokens = ['[CLS]']\n","        tokens = []\n","        cur_pos = 0\n","        pos1_in_index = 0\n","        pos2_in_index = 0\n","        for token in raw_tokens:\n","            token = token.lower()\n","            if cur_pos == pos_head[0]:\n","                tokens.append('[unused0]')\n","                pos1_in_index = len(tokens)\n","            if cur_pos == pos_tail[0]:\n","                tokens.append('[unused1]')\n","                pos2_in_index = len(tokens)\n","            tokens += self.tokenizer.tokenize(token)\n","            if cur_pos == pos_head[-1]:\n","                tokens.append('[unused2]')\n","            if cur_pos == pos_tail[-1]:\n","                tokens.append('[unused3]')\n","            cur_pos += 1\n","        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokens)\n","        \n","        return indexed_tokens"]},{"cell_type":"markdown","metadata":{"id":"RAWjmvU7QvaC"},"source":["## Fine-Tuning"]},{"cell_type":"markdown","metadata":{"id":"2nxFWLQffoLB"},"source":["Defining some parameters for training of the model"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Nh7ojhQcQ49C"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 0 ns (started: 2024-03-06 22:00:15 +00:00)\n"]}],"source":["trainN = 5\n","N = 5\n","K = 1\n","Q = 1\n","batch_size = 4\n","max_length = 3\n","hidden_size = 3\n","na_rate = 5\n","\n","val_step = 1000\n","train_iter = 1000\n","val_iter = 1000\n","test_iter = 1000\n","\n","ckpt = os.path.join(basepath, 'checkpoint_files','bert-pair-fewrel.pth.tar')\n","prefix = 'bert-pair-fewrel.pth.tar'\n"]},{"cell_type":"markdown","metadata":{"id":"NWYM_oUYfzAz"},"source":["Initializing sentence encoder and model for BERT Pair"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"jRN6DIdIRWrk"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["time: 2.56 s (started: 2024-03-06 22:00:15 +00:00)\n"]}],"source":["sentence_encoder = BERTPAIRSentenceEncoder('bert-base-uncased', max_length)\n","\n","model = Pair(sentence_encoder, hidden_size=hidden_size)\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading model pre-trained on blanks ...\n","time: 1.75 s (started: 2024-03-06 22:00:18 +00:00)\n"]}],"source":["checkpoint_path = os.path.join(basepath, 'checkpoint_files\\pretrain_checkpoint_BERT_1.pth.tar')\n","\n","print(\"Loading model pre-trained on blanks ...\")\n","checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n","model_dict = model.state_dict()\n","pretrained_dict = {k: v for k, v in checkpoint['state_dict'].items() if k in model_dict.keys()}\n","model_dict.update(pretrained_dict)\n"]},{"cell_type":"markdown","metadata":{"id":"seYo0E1Hf8vo"},"source":["Loading train-, validation- and test-data and initializing the FewShotREFramework with the different data loaders"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 16 ms (started: 2024-03-06 22:00:20 +00:00)\n"]}],"source":["import torch.utils.data as data\n","class FewRelDatasetPair(data.Dataset):\n","    \"\"\"\n","    FewRel Pair Dataset\n","    \"\"\"\n","    def __init__(self, name, encoder, N, K, Q, na_rate, root, encoder_name):\n","\n","        self.root = root\n","        path = os.path.join(root, name + \".json\")\n","        if not os.path.exists(path):\n","            print(\"[ERROR] Data file does not exist!\")\n","            assert(0)\n","\n","        self.json_data = json.load(open(path))\n","\n","        self.classes = list(self.json_data.keys())\n","        self.N = N\n","        self.K = K\n","        self.Q = Q\n","        self.na_rate = na_rate\n","        self.encoder = encoder\n","        self.encoder_name = encoder_name\n","        self.max_length = encoder.max_length\n","\n","    def __getraw__(self, item):\n","        word = self.encoder.tokenize(item['tokens'],\n","            item['h'][2][0],\n","            item['t'][2][0])\n","        return word \n","\n","    def __additem__(self, d, word, pos1, pos2, mask):\n","        d['word'].append(word)\n","        d['pos1'].append(pos1)\n","        d['pos2'].append(pos2)\n","        d['mask'].append(mask)\n","\n","    def __getitem__(self, index):\n","        target_classes = random.sample(self.classes, self.N)\n","        support = []\n","        query = []\n","        fusion_set = {'word': [], 'mask': [], 'seg': []}\n","        query_label = []\n","        Q_na = int(self.na_rate * self.Q)\n","        na_classes = list(filter(lambda x: x not in target_classes,  \n","            self.classes))\n","\n","        for i, class_name in enumerate(target_classes):\n","            indices = np.random.choice(\n","                    list(range(len(self.json_data[class_name]))), \n","                    self.K + self.Q, False)\n","            count = 0\n","            for j in indices:\n","                word  = self.__getraw__(\n","                        self.json_data[class_name][j])\n","                if count < self.K:\n","                    support.append(word)\n","                else:\n","                    query.append(word)\n","                count += 1\n","\n","            query_label += [i] * self.Q\n","\n","        # NA\n","        for j in range(Q_na):\n","            cur_class = np.random.choice(na_classes, 1, False)[0]\n","            index = np.random.choice(\n","                    list(range(len(self.json_data[cur_class]))),\n","                    1, False)[0]\n","            word = self.__getraw__(\n","                    self.json_data[cur_class][index])\n","            query.append(word)\n","        query_label += [self.N] * Q_na\n","\n","        for word_query in query:\n","            for word_support in support:\n","                if self.encoder_name == 'bert':\n","                    SEP = self.encoder.tokenizer.convert_tokens_to_ids(['[SEP]'])\n","                    CLS = self.encoder.tokenizer.convert_tokens_to_ids(['[CLS]'])\n","                    word_tensor = torch.zeros((self.max_length)).long()\n","                else:\n","                    SEP = self.encoder.tokenizer.convert_tokens_to_ids(['</s>'])     \n","                    CLS = self.encoder.tokenizer.convert_tokens_to_ids(['<s>'])\n","                    word_tensor = torch.ones((self.max_length)).long()\n","                new_word = CLS + word_support + SEP + word_query + SEP\n","                for i in range(min(self.max_length, len(new_word))):\n","                    word_tensor[i] = new_word[i]\n","                mask_tensor = torch.zeros((self.max_length)).long()\n","                mask_tensor[:min(self.max_length, len(new_word))] = 1\n","                seg_tensor = torch.ones((self.max_length)).long()\n","                seg_tensor[:min(self.max_length, len(word_support) + 1)] = 0\n","                fusion_set['word'].append(word_tensor)\n","                fusion_set['mask'].append(mask_tensor)\n","                fusion_set['seg'].append(seg_tensor)\n","\n","        return fusion_set, query_label\n","    \n","    def __len__(self):\n","        return 10000000"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"s83boei_LbND"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 0 ns (started: 2024-03-06 22:00:20 +00:00)\n"]}],"source":["def collate_fn_pair(data):\n","    batch_set = {'word': [], 'seg': [], 'mask': []}\n","    batch_label = []\n","    fusion_sets, query_labels = zip(*data)\n","    for i in range(len(fusion_sets)):\n","        for k in fusion_sets[i]:\n","            batch_set[k] += fusion_sets[i][k]\n","        batch_label += query_labels[i]\n","    for k in batch_set:\n","        batch_set[k] = torch.stack(batch_set[k], 0)\n","    batch_label = torch.tensor(batch_label)\n","    return batch_set, batch_label\n","\n","def get_loader_pair(name, encoder, N, K, Q, batch_size, \n","        num_workers=0, collate_fn=collate_fn_pair, na_rate=0, root='./data', encoder_name='bert'):\n","    dataset = FewRelDatasetPair(name, encoder, N, K, Q, na_rate, root, encoder_name)\n","    data_loader = torch.utils.data.DataLoader(dataset=dataset,\n","            batch_size=batch_size,\n","            shuffle=False,\n","            pin_memory=True,\n","            num_workers=num_workers,\n","            collate_fn=collate_fn)\n","    return iter(data_loader)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"aG4Ht8KJYsvb"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 953 ms (started: 2024-03-06 22:00:20 +00:00)\n"]}],"source":["train_data_loader = get_loader_pair(train_file, sentence_encoder, N=trainN, K=K, Q=Q, na_rate=na_rate, batch_size=batch_size, encoder_name='bert')\n","val_data_loader = get_loader_pair(val_file, sentence_encoder, N=N, K=K, Q=Q, na_rate=na_rate, batch_size=batch_size, encoder_name='bert')\n","test_data_loader = get_loader_pair(test_file, sentence_encoder, N=N, K=K, Q=Q, na_rate=na_rate, batch_size=batch_size, encoder_name='bert')\n","\n","framework = FewShotREFramework(train_data_loader, val_data_loader, test_data_loader)\n"]},{"cell_type":"markdown","metadata":{"id":"XJhP6ew3fe1-"},"source":["Training the model using the provided FewShotREFramework from the authors of the FewRel dataset"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"foOVYGzHUuf9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start training...\n","Use bert optim!\n","step:    1 | loss: 1.919570, accuracy: 10.00%\n","step:    2 | loss: 1.922436, accuracy: 7.50%\n","step:    3 | loss: 1.911219, accuracy: 10.83%\n","step:    4 | loss: 1.918554, accuracy: 10.63%\n","step:    5 | loss: 1.916086, accuracy: 10.50%\n","step:    6 | loss: 1.924606, accuracy: 9.17%\n","step:    7 | loss: 1.923395, accuracy: 8.57%\n","step:    8 | loss: 1.914458, accuracy: 10.00%\n","step:    9 | loss: 1.908876, accuracy: 11.11%\n","step:   10 | loss: 1.907902, accuracy: 11.25%\n","step:   11 | loss: 1.905148, accuracy: 10.91%\n","step:   12 | loss: 1.899234, accuracy: 11.25%\n","step:   13 | loss: 1.894172, accuracy: 11.35%\n","step:   14 | loss: 1.884616, accuracy: 12.32%\n","step:   15 | loss: 1.873880, accuracy: 12.67%\n","step:   16 | loss: 1.867641, accuracy: 13.59%\n","step:   17 | loss: 1.858376, accuracy: 14.85%\n","step:   18 | loss: 1.850629, accuracy: 16.67%\n","step:   19 | loss: 1.841219, accuracy: 18.29%\n","step:   20 | loss: 1.830702, accuracy: 19.62%\n","step:   21 | loss: 1.821647, accuracy: 20.83%\n","step:   22 | loss: 1.812670, accuracy: 21.93%\n","step:   23 | loss: 1.805360, accuracy: 23.26%\n","step:   24 | loss: 1.797046, accuracy: 24.27%\n","step:   25 | loss: 1.789187, accuracy: 25.10%\n","step:   26 | loss: 1.782067, accuracy: 26.06%\n","step:   27 | loss: 1.774493, accuracy: 26.94%\n","step:   28 | loss: 1.766667, accuracy: 27.77%\n","step:   29 | loss: 1.760025, accuracy: 28.53%\n","step:   30 | loss: 1.752754, accuracy: 29.17%\n","step:   31 | loss: 1.747336, accuracy: 29.76%\n","step:   32 | loss: 1.740619, accuracy: 30.39%\n","step:   33 | loss: 1.734161, accuracy: 30.98%\n","step:   34 | loss: 1.729008, accuracy: 31.54%\n","step:   35 | loss: 1.724059, accuracy: 32.07%\n","step:   36 | loss: 1.718078, accuracy: 32.57%\n","step:   37 | loss: 1.712650, accuracy: 33.04%\n","step:   38 | loss: 1.707334, accuracy: 33.49%\n","step:   39 | loss: 1.703248, accuracy: 33.85%\n","step:   40 | loss: 1.699334, accuracy: 34.25%\n","step:   41 | loss: 1.695190, accuracy: 34.63%\n","step:   42 | loss: 1.691219, accuracy: 35.00%\n","step:   43 | loss: 1.687429, accuracy: 35.35%\n","step:   44 | loss: 1.683627, accuracy: 35.68%\n","step:   45 | loss: 1.680518, accuracy: 36.00%\n","step:   46 | loss: 1.677657, accuracy: 36.30%\n","step:   47 | loss: 1.673531, accuracy: 36.60%\n","step:   48 | loss: 1.670253, accuracy: 36.87%\n","step:   49 | loss: 1.666420, accuracy: 37.14%\n","step:   50 | loss: 1.663217, accuracy: 37.40%\n","step:   51 | loss: 1.660741, accuracy: 37.65%\n","step:   52 | loss: 1.658510, accuracy: 37.88%\n","step:   53 | loss: 1.655103, accuracy: 38.11%\n","step:   54 | loss: 1.652222, accuracy: 38.33%\n","step:   55 | loss: 1.649558, accuracy: 38.55%\n","step:   56 | loss: 1.646393, accuracy: 38.75%\n","step:   57 | loss: 1.644588, accuracy: 38.95%\n","step:   58 | loss: 1.641927, accuracy: 39.14%\n","step:   59 | loss: 1.639748, accuracy: 39.32%\n","step:   60 | loss: 1.637255, accuracy: 39.50%\n","step:   61 | loss: 1.634949, accuracy: 39.67%\n","step:   62 | loss: 1.632849, accuracy: 39.84%\n","step:   63 | loss: 1.630961, accuracy: 40.00%\n","step:   64 | loss: 1.629583, accuracy: 40.16%\n","step:   65 | loss: 1.627334, accuracy: 40.31%\n","step:   66 | loss: 1.624935, accuracy: 40.45%\n","step:   67 | loss: 1.623328, accuracy: 40.60%\n","step:   68 | loss: 1.621475, accuracy: 40.74%\n","step:   69 | loss: 1.619651, accuracy: 40.87%\n","step:   70 | loss: 1.617592, accuracy: 41.00%\n","step:   71 | loss: 1.615698, accuracy: 41.13%\n","step:   72 | loss: 1.614163, accuracy: 41.25%\n","step:   73 | loss: 1.613163, accuracy: 41.37%\n","step:   74 | loss: 1.611407, accuracy: 41.49%\n","step:   75 | loss: 1.609945, accuracy: 41.60%\n","step:   76 | loss: 1.608313, accuracy: 41.71%\n","step:   77 | loss: 1.607023, accuracy: 41.82%\n","step:   78 | loss: 1.605733, accuracy: 41.92%\n","step:   79 | loss: 1.604552, accuracy: 42.03%\n","step:   80 | loss: 1.603383, accuracy: 42.12%\n","step:   81 | loss: 1.602064, accuracy: 42.22%\n","step:   82 | loss: 1.600980, accuracy: 42.32%\n","step:   83 | loss: 1.599876, accuracy: 42.41%\n","step:   84 | loss: 1.598644, accuracy: 42.50%\n","step:   85 | loss: 1.597623, accuracy: 42.59%\n","step:   86 | loss: 1.596663, accuracy: 42.67%\n","step:   87 | loss: 1.595718, accuracy: 42.76%\n","step:   88 | loss: 1.594901, accuracy: 42.84%\n","step:   89 | loss: 1.594027, accuracy: 42.92%\n","step:   90 | loss: 1.592982, accuracy: 43.00%\n","step:   91 | loss: 1.592125, accuracy: 43.08%\n","step:   92 | loss: 1.591205, accuracy: 43.15%\n","step:   93 | loss: 1.590382, accuracy: 43.23%\n","step:   94 | loss: 1.589712, accuracy: 43.30%\n","step:   95 | loss: 1.588684, accuracy: 43.37%\n","step:   96 | loss: 1.587770, accuracy: 43.44%\n","step:   97 | loss: 1.587102, accuracy: 43.51%\n","step:   98 | loss: 1.586180, accuracy: 43.57%\n","step:   99 | loss: 1.585473, accuracy: 43.64%\n","step:  100 | loss: 1.584742, accuracy: 43.70%\n","step:  101 | loss: 1.584016, accuracy: 43.76%\n","step:  102 | loss: 1.583037, accuracy: 43.82%\n","step:  103 | loss: 1.582107, accuracy: 43.88%\n","step:  104 | loss: 1.581261, accuracy: 43.94%\n","step:  105 | loss: 1.580782, accuracy: 44.00%\n","step:  106 | loss: 1.580052, accuracy: 44.06%\n","step:  107 | loss: 1.579319, accuracy: 44.11%\n","step:  108 | loss: 1.578565, accuracy: 44.17%\n","step:  109 | loss: 1.578029, accuracy: 44.22%\n","step:  110 | loss: 1.577124, accuracy: 44.27%\n","step:  111 | loss: 1.576481, accuracy: 44.32%\n","step:  112 | loss: 1.575757, accuracy: 44.37%\n","step:  113 | loss: 1.575390, accuracy: 44.42%\n","step:  114 | loss: 1.574841, accuracy: 44.47%\n","step:  115 | loss: 1.574157, accuracy: 44.52%\n","step:  116 | loss: 1.573752, accuracy: 44.57%\n","step:  117 | loss: 1.573421, accuracy: 44.62%\n","step:  118 | loss: 1.572830, accuracy: 44.66%\n","step:  119 | loss: 1.572104, accuracy: 44.71%\n","step:  120 | loss: 1.571706, accuracy: 44.75%\n","step:  121 | loss: 1.571241, accuracy: 44.79%\n"]}],"source":["framework.train(model, prefix, batch_size, trainN, N, K, Q,\n","        pytorch_optim=optim.SGD, na_rate=na_rate, val_step=val_step, pair=True, \n","        train_iter=train_iter, val_iter=val_iter, bert_optim=True,\n","        save_ckpt=ckpt, load_ckpt=pretrained_dict)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMo8XTsfKLEzXTJjW54y5wo","collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1H0MsHAw6tTnoH6SUzELHAdoXHxVD0vkV","name":"BERT PAIR Relation Extraction.ipynb","provenance":[{"file_id":"1S-npM9Umn_s4D3FS6Fyl_pEo5fopcuob","timestamp":1591960614367},{"file_id":"1_i6d5uaAiqpuUwvf5NiHxcXHf58kAqt1","timestamp":1591254255357}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
